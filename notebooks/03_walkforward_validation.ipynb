{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45efd35",
   "metadata": {},
   "source": [
    "# 03 — Walk-Forward Validation (Rolling Cointegration + Out-of-Sample Backtest)\n",
    "\n",
    "This notebook implements the **“Next improvements”** section from the README:\n",
    "\n",
    "- **Walk-forward validation** with **rolling cointegration tests**\n",
    "- **Out-of-sample (OOS)** trading simulation for selected pairs\n",
    "- (Optional hooks) dynamic hedge ratio, regime filters, transaction costs\n",
    "\n",
    "> **Why this notebook exists:** Cointegration at a single point in time can look “significant,” yet fail in live trading because relationships drift. Walk-forward validation tests whether a pair remains tradable **out of sample**, not just in-sample.\n",
    "\n",
    "---\n",
    "\n",
    "## What you’ll get\n",
    "\n",
    "1. **Rolling windows** (train → trade) across time  \n",
    "2. For each window:\n",
    "   - Select pair(s) by Engle–Granger p-value on the **train** period\n",
    "   - Estimate hedge ratio (**beta**) on train (OLS)\n",
    "   - Compute spread + z-score using train statistics\n",
    "   - Trade **only** in the **next OOS** period\n",
    "3. Aggregate OOS equity curve + performance metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## Assumptions / Conventions\n",
    "\n",
    "- Price input: **Adjusted Close**\n",
    "- Spread: \\( s_t = p^A_t - \\beta p^B_t \\)\n",
    "- Z-score uses **train mean/std** (or rolling in trade, configurable)\n",
    "- Entry/exit thresholds default to the README: entry \\(|z|>2\\), exit \\(|z|<0.5\\)\n",
    "- Position sizing: 1x notional per leg (dollar-neutral), single pair at a time (configurable)\n",
    "\n",
    "---\n",
    "\n",
    "## Data caching (no parquet)\n",
    "\n",
    "This repo previously hit a parquet ImportError when `pyarrow/fastparquet` aren’t installed.  \n",
    "This notebook uses **CSV caching** by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee96334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're running from the repo root, paths will just work.\n",
    "# If not, adjust PROJECT_ROOT accordingly.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import coint, adfuller\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "CACHE_DIR = DATA_DIR / \"cache\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"CACHE_DIR:\", CACHE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa40bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading ---\n",
    "# This notebook supports:\n",
    "# 1) Pulling prices from yfinance (most convenient)\n",
    "# 2) Loading an existing CSV cache\n",
    "\n",
    "TICKERS = [\"NVDA\", \"JPM\", \"AMZN\", \"META\"]  # update freely\n",
    "START = \"2016-01-01\"\n",
    "END = None  # None = today\n",
    "\n",
    "PRICE_CACHE_CSV = CACHE_DIR / f\"adj_close_{START}_to_{END or 'today'}_{'-'.join(TICKERS)}.csv\"\n",
    "\n",
    "def download_adj_close_yf(tickers: list[str], start: str, end: str | None) -> pd.DataFrame:\n",
    "    import yfinance as yf\n",
    "    df = yf.download(\n",
    "        tickers,\n",
    "        start=start,\n",
    "        end=end,\n",
    "        auto_adjust=False,\n",
    "        progress=False\n",
    "    )\n",
    "    # yfinance returns columns like ('Adj Close', 'NVDA') if multiple tickers\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        out = df[\"Adj Close\"].copy()\n",
    "    else:\n",
    "        out = df.rename(columns={\"Adj Close\": tickers[0]})[tickers[0]].to_frame()\n",
    "    out.index = pd.to_datetime(out.index)\n",
    "    out = out.sort_index()\n",
    "    out = out.dropna(how=\"all\")\n",
    "    return out\n",
    "\n",
    "def load_prices(tickers: list[str], start: str, end: str | None, cache_path: Path) -> pd.DataFrame:\n",
    "    if cache_path.exists():\n",
    "        prices = pd.read_csv(cache_path, index_col=0, parse_dates=True)\n",
    "        # keep only requested tickers if cache has more\n",
    "        missing = [t for t in tickers if t not in prices.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Cache exists but missing tickers: {missing}. Delete cache or update tickers.\")\n",
    "        return prices[tickers].copy()\n",
    "    prices = download_adj_close_yf(tickers, start, end)\n",
    "    prices.to_csv(cache_path)\n",
    "    print(f\"Saved cache -> {cache_path}\")\n",
    "    return prices\n",
    "\n",
    "prices = load_prices(TICKERS, START, END, PRICE_CACHE_CSV)\n",
    "prices = prices.dropna()\n",
    "prices.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cabea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity checks\n",
    "prices.describe().T[[\"count\", \"mean\", \"std\", \"min\", \"max\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633c5384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prices (log scale helps for long horizons)\n",
    "ax = (np.log(prices)).plot(figsize=(12, 5), title=\"Log Adj Close\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2383e92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helpers: cointegration, hedge ratio, z-score, backtest ---\n",
    "\n",
    "def ols_beta(y: pd.Series, x: pd.Series) -> float:\n",
    "    \"\"\"Estimate beta from y ~ alpha + beta*x via OLS.\"\"\"\n",
    "    df = pd.concat([y, x], axis=1).dropna()\n",
    "    yv = df.iloc[:, 0].values\n",
    "    xv = sm.add_constant(df.iloc[:, 1].values)\n",
    "    model = sm.OLS(yv, xv).fit()\n",
    "    return float(model.params[1])\n",
    "\n",
    "def spread_series(pA: pd.Series, pB: pd.Series, beta: float) -> pd.Series:\n",
    "    return pA - beta * pB\n",
    "\n",
    "def zscore_from_train(spread: pd.Series, train_mask: pd.Series, eps: float = 1e-12) -> pd.Series:\n",
    "    mu = spread[train_mask].mean()\n",
    "    sd = spread[train_mask].std(ddof=0)\n",
    "    sd = max(sd, eps)\n",
    "    return (spread - mu) / sd\n",
    "\n",
    "def coint_pvalue(y: pd.Series, x: pd.Series) -> float:\n",
    "    df = pd.concat([y, x], axis=1).dropna()\n",
    "    if len(df) < 60:\n",
    "        return np.nan\n",
    "    score, pvalue, _ = coint(df.iloc[:, 0], df.iloc[:, 1])\n",
    "    return float(pvalue)\n",
    "\n",
    "def adf_pvalue(s: pd.Series) -> float:\n",
    "    s = s.dropna()\n",
    "    if len(s) < 60:\n",
    "        return np.nan\n",
    "    res = adfuller(s, autolag=\"AIC\")\n",
    "    return float(res[1])\n",
    "\n",
    "def backtest_pair_oos(\n",
    "    pA: pd.Series,\n",
    "    pB: pd.Series,\n",
    "    train_mask: pd.Series,\n",
    "    trade_mask: pd.Series,\n",
    "    entry_z: float = 2.0,\n",
    "    exit_z: float = 0.5,\n",
    "    cost_bps: float = 0.0,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Dollar-neutral backtest trading only during trade_mask, signals calibrated on train_mask.\n",
    "\n",
    "    Positions:\n",
    "      - If z > entry: short spread (short A, long beta*B)\n",
    "      - If z < -entry: long spread (long A, short beta*B)\n",
    "      - Exit when |z| < exit_z\n",
    "    Transaction cost: 'cost_bps' per *gross* notional turn (simple approximation).\n",
    "    \"\"\"\n",
    "    # Estimate beta on train\n",
    "    beta = ols_beta(pA[train_mask], pB[train_mask])\n",
    "\n",
    "    spr = spread_series(pA, pB, beta).rename(\"spread\")\n",
    "    z = zscore_from_train(spr, train_mask).rename(\"z\")\n",
    "\n",
    "    idx = spr.index\n",
    "    pos = pd.Series(0.0, index=idx)  # +1 long spread, -1 short spread\n",
    "    state = 0.0\n",
    "\n",
    "    # only generate trades inside trade period; stay flat outside\n",
    "    for t in idx:\n",
    "        if not trade_mask.loc[t]:\n",
    "            state = 0.0\n",
    "            pos.loc[t] = 0.0\n",
    "            continue\n",
    "\n",
    "        zt = z.loc[t]\n",
    "        if state == 0.0:\n",
    "            if zt <= -entry_z:\n",
    "                state = +1.0\n",
    "            elif zt >= entry_z:\n",
    "                state = -1.0\n",
    "        else:\n",
    "            # exit rule\n",
    "            if abs(zt) <= exit_z:\n",
    "                state = 0.0\n",
    "        pos.loc[t] = state\n",
    "\n",
    "    # returns: approximate dollar-neutral PnL using price changes\n",
    "    # spread return proxy: Δ(A - beta*B)\n",
    "    ds = spr.diff().fillna(0.0)\n",
    "\n",
    "    pnl = pos.shift(1).fillna(0.0) * ds  # enter at next bar\n",
    "    pnl = pnl.rename(\"pnl\")\n",
    "\n",
    "    # simple costs: whenever position changes, pay cost_bps on gross notional\n",
    "    turnover = pos.diff().abs().fillna(0.0).rename(\"turnover\")\n",
    "    # Gross notional ~ 1 + |beta| (one unit A plus beta units of B); scale cost accordingly.\n",
    "    gross = (1.0 + abs(beta))\n",
    "    cost = (turnover * (cost_bps / 10_000.0) * gross).rename(\"cost\")\n",
    "\n",
    "    net = (pnl - cost).rename(\"net_pnl\")\n",
    "\n",
    "    out = pd.concat([spr, z, pos.rename(\"position\"), pnl, cost, net], axis=1)\n",
    "    out[\"beta\"] = beta\n",
    "    out[\"train_mean_spread\"] = spr[train_mask].mean()\n",
    "    out[\"train_std_spread\"] = spr[train_mask].std(ddof=0)\n",
    "    out[\"is_train\"] = train_mask.astype(int)\n",
    "    out[\"is_trade\"] = trade_mask.astype(int)\n",
    "    return out\n",
    "\n",
    "def perf_stats(equity: pd.Series, freq: int = 252) -> dict:\n",
    "    r = equity.pct_change().dropna()\n",
    "    ann_ret = (equity.iloc[-1] ** (freq / max(len(r), 1)) - 1.0) if len(r) else np.nan\n",
    "    ann_vol = r.std(ddof=0) * np.sqrt(freq) if len(r) else np.nan\n",
    "    sharpe = (r.mean() / (r.std(ddof=0) + 1e-12)) * np.sqrt(freq) if len(r) else np.nan\n",
    "\n",
    "    # max drawdown\n",
    "    peak = equity.cummax()\n",
    "    dd = (equity / peak) - 1.0\n",
    "    max_dd = dd.min() if len(dd) else np.nan\n",
    "\n",
    "    win_rate = (r > 0).mean() if len(r) else np.nan\n",
    "\n",
    "    return {\n",
    "        \"annualized_return\": float(ann_ret) if np.isfinite(ann_ret) else np.nan,\n",
    "        \"annualized_vol\": float(ann_vol) if np.isfinite(ann_vol) else np.nan,\n",
    "        \"sharpe\": float(sharpe) if np.isfinite(sharpe) else np.nan,\n",
    "        \"max_drawdown\": float(max_dd) if np.isfinite(max_dd) else np.nan,\n",
    "        \"win_rate_daily\": float(win_rate) if np.isfinite(win_rate) else np.nan,\n",
    "        \"equity_start\": float(equity.iloc[0]) if len(equity) else np.nan,\n",
    "        \"equity_end\": float(equity.iloc[-1]) if len(equity) else np.nan,\n",
    "        \"n_days\": int(len(equity)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e04185",
   "metadata": {},
   "source": [
    "## Walk-forward design\n",
    "\n",
    "We split the timeline into repeated **train → trade** windows:\n",
    "\n",
    "- `train_window_days`: length used for **pair selection** and calibration (beta + spread mean/std)\n",
    "- `trade_window_days`: subsequent OOS window where we **actually trade**\n",
    "- `step_days`: how much we slide forward each iteration\n",
    "\n",
    "Typical choices:\n",
    "- train: 252–756 trading days\n",
    "- trade: 63–252 trading days\n",
    "- step: trade_window_days (non-overlapping) or smaller (overlapping)\n",
    "\n",
    "Below we implement a flexible walk-forward runner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdce3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# --- Walk-forward configuration ---\n",
    "TRAIN_WINDOW_DAYS = 504   # ~2 years\n",
    "TRADE_WINDOW_DAYS = 126   # ~6 months\n",
    "STEP_DAYS = 126           # slide by 6 months\n",
    "\n",
    "ENTRY_Z = 2.0\n",
    "EXIT_Z = 0.5\n",
    "COST_BPS = 0.0  # set e.g., 1-5 bps if you want a simple friction term\n",
    "\n",
    "# pair selection criteria on TRAIN window\n",
    "MAX_COINTEGRATION_P = 0.05\n",
    "TOP_K_PAIRS_PER_WINDOW = 1  # trade best pair per window (ranked by p-value)\n",
    "\n",
    "all_pairs = list(combinations(prices.columns.tolist(), 2))\n",
    "all_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b975a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_windows(index: pd.DatetimeIndex, train_days: int, trade_days: int, step_days: int):\n",
    "    # Use integer positions (trading days) rather than calendar days\n",
    "    n = len(index)\n",
    "    start = 0\n",
    "    while True:\n",
    "        train_start = start\n",
    "        train_end = train_start + train_days\n",
    "        trade_end = train_end + trade_days\n",
    "        if trade_end > n:\n",
    "            break\n",
    "        yield (train_start, train_end, trade_end)\n",
    "        start += step_days\n",
    "\n",
    "def select_pairs_train(prices_slice: pd.DataFrame, pairs: list[tuple[str, str]], max_p: float, top_k: int):\n",
    "    rows = []\n",
    "    for a, b in pairs:\n",
    "        p = coint_pvalue(prices_slice[a], prices_slice[b])\n",
    "        rows.append((a, b, p))\n",
    "    sel = pd.DataFrame(rows, columns=[\"asset_A\", \"asset_B\", \"coint_pvalue\"]).dropna()\n",
    "    sel = sel.sort_values(\"coint_pvalue\", ascending=True)\n",
    "    sel = sel[sel[\"coint_pvalue\"] <= max_p].head(top_k)\n",
    "    return sel.reset_index(drop=True)\n",
    "\n",
    "# Run walk-forward\n",
    "windows = list(iter_windows(prices.index, TRAIN_WINDOW_DAYS, TRADE_WINDOW_DAYS, STEP_DAYS))\n",
    "print(f\"Num windows: {len(windows)}\")\n",
    "\n",
    "window_summaries = []\n",
    "oos_trades = []  # store detailed trade frames per window\n",
    "\n",
    "for w, (i0, i1, i2) in enumerate(windows, start=1):\n",
    "    idx = prices.index\n",
    "    train_idx = idx[i0:i1]\n",
    "    trade_idx = idx[i1:i2]\n",
    "\n",
    "    train_mask = prices.index.isin(train_idx)\n",
    "    trade_mask = prices.index.isin(trade_idx)\n",
    "\n",
    "    train_prices = prices.loc[train_idx]\n",
    "\n",
    "    selected = select_pairs_train(train_prices, all_pairs, MAX_COINTEGRATION_P, TOP_K_PAIRS_PER_WINDOW)\n",
    "    if selected.empty:\n",
    "        window_summaries.append({\n",
    "            \"window\": w,\n",
    "            \"train_start\": train_idx[0],\n",
    "            \"train_end\": train_idx[-1],\n",
    "            \"trade_start\": trade_idx[0],\n",
    "            \"trade_end\": trade_idx[-1],\n",
    "            \"selected_pairs\": 0,\n",
    "            \"note\": \"No pairs passed cointegration threshold\"\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # Trade each selected pair, then combine (here: equal-weight if >1)\n",
    "    pair_equities = []\n",
    "    pair_meta = []\n",
    "\n",
    "    for _, row in selected.iterrows():\n",
    "        a, b = row[\"asset_A\"], row[\"asset_B\"]\n",
    "        bt = backtest_pair_oos(\n",
    "            prices[a],\n",
    "            prices[b],\n",
    "            train_mask=pd.Series(train_mask, index=prices.index),\n",
    "            trade_mask=pd.Series(trade_mask, index=prices.index),\n",
    "            entry_z=ENTRY_Z,\n",
    "            exit_z=EXIT_Z,\n",
    "            cost_bps=COST_BPS,\n",
    "        )\n",
    "\n",
    "        # Equity during trade period only; start at 1.0 per window/pair\n",
    "        trade_bt = bt.loc[trade_idx].copy()\n",
    "        eq = (1.0 + trade_bt[\"net_pnl\"]).cumprod()\n",
    "        eq.name = f\"equity_{a}_{b}\"\n",
    "        pair_equities.append(eq)\n",
    "\n",
    "        # Add diagnostics\n",
    "        spr_adf_p = adf_pvalue(bt.loc[train_idx, \"spread\"])\n",
    "        pair_meta.append({\n",
    "            \"asset_A\": a,\n",
    "            \"asset_B\": b,\n",
    "            \"coint_pvalue_train\": float(row[\"coint_pvalue\"]),\n",
    "            \"adf_pvalue_spread_train\": spr_adf_p,\n",
    "            \"beta_train\": float(bt[\"beta\"].iloc[0]),\n",
    "        })\n",
    "\n",
    "        # keep full detail for debugging\n",
    "        bt[\"window\"] = w\n",
    "        bt[\"asset_A\"] = a\n",
    "        bt[\"asset_B\"] = b\n",
    "        oos_trades.append(bt)\n",
    "\n",
    "    # Combine if multiple pairs (equal weight on equity curves)\n",
    "    equity_mat = pd.concat(pair_equities, axis=1).fillna(method=\"ffill\").fillna(1.0)\n",
    "    # average equity across pairs (rough equal-weight)\n",
    "    combined = equity_mat.mean(axis=1).rename(\"equity_window\")\n",
    "\n",
    "    stats = perf_stats(combined)\n",
    "    window_summaries.append({\n",
    "        \"window\": w,\n",
    "        \"train_start\": train_idx[0],\n",
    "        \"train_end\": train_idx[-1],\n",
    "        \"trade_start\": trade_idx[0],\n",
    "        \"trade_end\": trade_idx[-1],\n",
    "        \"selected_pairs\": len(pair_meta),\n",
    "        \"pairs\": pair_meta,\n",
    "        **stats,\n",
    "    })\n",
    "\n",
    "window_summary_df = pd.DataFrame(window_summaries)\n",
    "window_summary_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9622bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How often did we find a cointegrated pair in the training window?\n",
    "window_summary_df[\"selected_pairs\"].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059d4ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build overall OOS equity curve by chaining window returns ---\n",
    "# For windows with no selected pair, equity stays flat in that period.\n",
    "\n",
    "idx = prices.index\n",
    "overall = pd.Series(1.0, index=idx, name=\"equity_oos\")\n",
    "\n",
    "# We'll fill per-trade-window equity from the combined per-window result.\n",
    "# To do that, we rebuild per-window combined equity from stored oos_trades.\n",
    "oos_trades_df = pd.concat(oos_trades, axis=0) if oos_trades else pd.DataFrame()\n",
    "oos_trades_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_equity_from_trades(trades: pd.DataFrame, trade_idx: pd.DatetimeIndex) -> pd.Series:\n",
    "    # If multiple pairs in a window, average their equity curves\n",
    "    eqs = []\n",
    "    for (a, b), df in trades.groupby([\"asset_A\", \"asset_B\"]):\n",
    "        d = df.loc[trade_idx].copy()\n",
    "        eq = (1.0 + d[\"net_pnl\"]).cumprod()\n",
    "        eqs.append(eq.rename(f\"{a}_{b}\"))\n",
    "    if not eqs:\n",
    "        return pd.Series(1.0, index=trade_idx, name=\"equity_window\")\n",
    "    mat = pd.concat(eqs, axis=1).fillna(method=\"ffill\").fillna(1.0)\n",
    "    return mat.mean(axis=1).rename(\"equity_window\")\n",
    "\n",
    "overall = pd.Series(1.0, index=prices.index, name=\"equity_oos\")\n",
    "\n",
    "if not oos_trades_df.empty:\n",
    "    for w, (i0, i1, i2) in enumerate(windows, start=1):\n",
    "        trade_idx = prices.index[i1:i2]\n",
    "        w_trades = oos_trades_df[oos_trades_df[\"window\"] == w]\n",
    "        eq_w = window_equity_from_trades(w_trades, trade_idx)\n",
    "        # chain multiplicatively from last value before trade window\n",
    "        prev = overall.loc[trade_idx[0] - pd.Timedelta(days=1)] if trade_idx[0] - pd.Timedelta(days=1) in overall.index else overall.loc[:trade_idx[0]].iloc[-1]\n",
    "        # normalize window equity to start at 1.0 then chain\n",
    "        overall.loc[trade_idx] = prev * (eq_w / eq_w.iloc[0])\n",
    "\n",
    "overall = overall.ffill()\n",
    "overall.head(), overall.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4035bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot OOS equity curve\n",
    "ax = overall.plot(figsize=(12, 5), title=\"Walk-Forward OOS Equity Curve\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Equity\")\n",
    "plt.show()\n",
    "\n",
    "# Drawdown\n",
    "peak = overall.cummax()\n",
    "dd = overall / peak - 1.0\n",
    "ax = dd.plot(figsize=(12, 4), title=\"Drawdown (OOS)\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Drawdown\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19486cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio-level OOS metrics\n",
    "oos_stats = perf_stats(overall)\n",
    "pd.DataFrame([oos_stats]).T.rename(columns={0: \"value\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542ed7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window-by-window diagnostics (sort by Sharpe)\n",
    "cols = [\n",
    "    \"window\",\"train_start\",\"train_end\",\"trade_start\",\"trade_end\",\"selected_pairs\",\n",
    "    \"annualized_return\",\"sharpe\",\"max_drawdown\",\"win_rate_daily\",\"equity_end\"\n",
    "]\n",
    "window_summary_df[cols].sort_values(\"sharpe\", ascending=False).head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856afa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export results ---\n",
    "OUT_DIR = PROJECT_ROOT / \"outputs\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "window_summary_path = OUT_DIR / \"walkforward_window_summary.csv\"\n",
    "equity_path = OUT_DIR / \"walkforward_oos_equity.csv\"\n",
    "trades_path = OUT_DIR / \"walkforward_oos_trades.csv\"\n",
    "\n",
    "window_summary_df.to_csv(window_summary_path, index=False)\n",
    "overall.to_frame().to_csv(equity_path, index=True)\n",
    "\n",
    "if not oos_trades_df.empty:\n",
    "    # keep only trade rows to reduce file size\n",
    "    oos_trade_rows = oos_trades_df[oos_trades_df[\"is_trade\"] == 1].copy()\n",
    "    oos_trade_rows.to_csv(trades_path, index=True)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"-\", window_summary_path)\n",
    "print(\"-\", equity_path)\n",
    "print(\"-\", trades_path if not oos_trades_df.empty else \"(no trades to save)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f588f0d0",
   "metadata": {},
   "source": [
    "## Extensions (Optional)\n",
    "\n",
    "If you want to go beyond the README improvements:\n",
    "\n",
    "### 1) Rolling/Adaptive hedge ratio\n",
    "- Replace static OLS beta with:\n",
    "  - Rolling OLS (re-estimate beta every N days using last M days)\n",
    "  - Kalman filter (time-varying beta)\n",
    "\n",
    "### 2) Regime filters\n",
    "- Volatility filter: trade only when realized vol is below a threshold\n",
    "- Trend filter: avoid mean-reversion bets when the spread is trending\n",
    "\n",
    "### 3) Transaction costs + slippage\n",
    "- Model per-share costs, half-spread, and market impact\n",
    "- Costs matter a lot for high-turnover strategies\n",
    "\n",
    "### 4) Portfolio constraints\n",
    "- Multiple pairs concurrently\n",
    "- Gross/net exposure caps\n",
    "- Risk parity sizing\n",
    "\n",
    "If you want, I can add any of the above as extra sections in this notebook\n",
    "(or as **04_dynamic_hedge_ratio_kalman.ipynb**).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
